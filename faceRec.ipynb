{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faceRec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgaMySCxgwTbh2bxbKLkPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Williamsjanthony15/Projects/blob/main/faceRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZr542yHJIkY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg41n0GJJoq6"
      },
      "source": [
        "def faceRec(s):\n",
        "  n = 0\n",
        "  for i in s:\n",
        "    n = n*10 + ord(i) - ord(\"0\")\n",
        "  return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjeMui5AJzzS"
      },
      "source": [
        "mainFolders = [\"test\", \"train\"]\n",
        "emotFolders = ['angry', 'sad', 'happy', 'fearful', 'surprised']\n",
        "os.makedirs('data', exist_ok=True)\n",
        "for mainFolders in mainFolders:\n",
        "  os.makedirs(os.path.join(\"data\",mainFolders), exist_ok=True)\n",
        "  for emotFolders in emotFolders:\n",
        "    os.makedirs(os.path.join('data', mainFolders, emotFolders), exist_ok=True)\n",
        "\n",
        "angry = 0\n",
        "sad = 0\n",
        "happy = 0\n",
        "fearful = 0\n",
        "surprised = 0\n",
        "angry_test = 0\n",
        "sad_test = 0\n",
        "happy_test = 0\n",
        "fearful_test = 0\n",
        "surprised_test = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbO7qHdhK1vC",
        "outputId": "239d13bc-043e-4116-a0b9-8c978d3598df"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")\n",
        "df = pd.read_csv('./fer2013.csv')\n",
        "mat = np.zeros((48,48), dtype=np.uint8)\n",
        "print(\"here i am saving pictures\")\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "  txt = df['pixels'][i]\n",
        "  words = txt.split()\n",
        "\n",
        "  for j in range(2304):\n",
        "    xind = j // 48\n",
        "    yind = j % 48\n",
        "    # mat[xind][yind] = faceRec(words[i])\n",
        "\n",
        "  img = Image.fromarray(mat)\n",
        "\n",
        "  if i < 28709:\n",
        "    if df['emotion'][i] == 0:\n",
        "      img.save('./data/train/angry/im'+str(angry)+'.png')\n",
        "      angry += 1\n",
        "    if df['emotion'][i] == 0:\n",
        "      img.save('./data/train/sad/im'+str(sad)+'.png')\n",
        "      sad += 1\n",
        "    if df['emotion'][i] == 0:\n",
        "      img.save('./data/train/happy/im'+str(happy)+'.png')\n",
        "      happy += 1\n",
        "    if df['emotion'][i] == 0:\n",
        "      img.save('./data/train/fearful/im'+str(fearful)+'.png')\n",
        "      fearful += 1\n",
        "    if df['emotion'][i] == 0:\n",
        "      img.save('./data/train/surprised/im'+str(surprised)+'.png')\n",
        "      surprised += 1\n",
        "\n",
        "  else:\n",
        "    if df['emotion'][i] == 0:\n",
        "      img.save('./data/test/angry/im'+str(angry_test)+'.png')\n",
        "      angry_test += 1\n",
        "    elif df['emotion'][i] == 0:\n",
        "      img.save('./data/test/sad/im'+str(sad_test)+'.png')\n",
        "      sad_test += 1\n",
        "    elif df['emotion'][i] == 0:\n",
        "      img.save('./data/test/happy/im'+str(happy_test)+'.png')\n",
        "      happy_test += 1\n",
        "    elif df['emotion'][i] == 0:\n",
        "      img.save('./data/test/fearful/im'+str(fearful_test)+'.png')\n",
        "      fearful_test += 1\n",
        "    elif df['emotion'][i] == 0:\n",
        "      img.save('./data/test/surprised/im'+str(surprised_test)+'.png')\n",
        "      surprised_test += 1\n",
        "\n",
        "print('done son!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here i am saving pictures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35887/35887 [00:32<00:00, 1093.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done son!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAagtfwTN3V-"
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdhKLK2OlEC"
      },
      "source": [
        "import argparse\n",
        "\n",
        "def arguments():\n",
        "  ap = argparse.ArgumentParser('--mode', help='train/display')\n",
        "# ap.add_argument('--mode',help='train/display')\n",
        "  mode = ap.parse_args().mode\n",
        "  return mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsK6HXYZO0D8",
        "outputId": "2ebb4b8b-29fa-4828-89d1-f454c751ed7f"
      },
      "source": [
        "train = 'data/train'\n",
        "validate = 'data/test'\n",
        "\n",
        "numTrain = 28709\n",
        "numValidate = 7178\n",
        "batch_size = 64\n",
        "num_epoch = 50\n",
        "\n",
        "datagenTrain = ImageDataGenerator(rescale=1./255)\n",
        "datagenValidate = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "trainGenerator = datagenTrain.flow_from_directory(\n",
        "    train,\n",
        "    target_size=(48, 48),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode=\"categorical\"\n",
        "    )\n",
        "\n",
        "validateGenerator = datagenValidate.flow_from_directory(\n",
        "    validate,\n",
        "    target_size=(48, 48),\n",
        "    batch_size = batch_size,\n",
        "    color_mode = \"grayscale\",\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31555 images belonging to 14 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WbgbWoGQFey"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QFfkUSbQOGn"
      },
      "source": [
        "if arguments == \"train\":\n",
        "  model.compile(loss='categorical_crossentrophy', optimize=Adam(lr=0.0001, decay=ie-6), metrics=['accuracy'])\n",
        "  model_info = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=numTrain // batch,\n",
        "      epochs = num_epoch,\n",
        "      validation_data = datagenValidate,\n",
        "      validation_steps = numValidate // batch\n",
        "  )\n",
        "  plot_model_history(model_info)\n",
        "  model.save_weights('model.h5')\n",
        "\n",
        "elif arguments == \"display\":\n",
        "    model.load_weights('model.h5')\n",
        "    cv2.ocl.setUseOpenCL(False)\n",
        "    emotionDict = {0: \"Angry\", 1: \"Sad\", 2: \"Happy\", 3: \"Fearful\", 4: \"Surprised\"}\n",
        "\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      fasecasc = cv2.CasadeClassifier('haarcascade_frontalface_defualt.xml')\n",
        "      gray = cv2.cvtColor(frame, cv2.Color_BGR2GRAY)\n",
        "      faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors = 5)\n",
        "\n",
        "      for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "        prediction = model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(prediction))\n",
        "        cv2.putText(frame, emotionDict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "      cv2.imshow('Video', cv2.resize(frame,(1600, 960),interpolation = cv2.INTER_CUBIC))\n",
        "      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}